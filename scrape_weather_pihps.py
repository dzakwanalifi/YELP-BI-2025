"""
Script untuk scraping historical weather data dari Open-Meteo API
untuk semua tanggal dan lokasi di data PIHPS.

Menggunakan multiprocessing untuk mempercepat scraping.

Author: Generated by Claude Code
Date: 2025-11-28
"""

import requests
import pandas as pd
from datetime import datetime, timedelta
import time
from typing import List, Dict, Optional
import logging
from multiprocessing import Pool, cpu_count
from tqdm import tqdm
import os

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# Koordinat untuk setiap lokasi (lat, lon)
LOCATION_COORDINATES = {
    # DKI Jakarta
    'DKI Jakarta': (-6.2088, 106.8456),
    'Jakarta Pusat': (-6.1862, 106.8346),

    # Jawa Barat
    'Jawa Barat': (-6.9175, 107.6191),  # Bandung sebagai representasi
    'Bandung': (-6.9175, 107.6191),
    'Bekasi': (-6.2383, 106.9756),
    'Bogor': (-6.5950, 106.7890),
    'Cirebon': (-6.7063, 108.5571),
    'Depok': (-6.4025, 106.7942),
    'Kab. Cirebon': (-6.7405, 108.3402),
    'Kab. Tasikmalaya': (-7.3506, 108.2050),
    'Sukabumi': (-6.9278, 106.9271),
    'Tasikmalaya': (-7.3274, 108.2207),
}


class OpenMeteoScraper:
    """Scraper untuk historical weather data dari Open-Meteo"""

    BASE_URL = "https://archive-api.open-meteo.com/v1/archive"

    # Weather variables yang diambil
    WEATHER_VARS = [
        "temperature_2m_max",
        "temperature_2m_min",
        "temperature_2m_mean",
        "precipitation_sum",
        "rain_sum",
        "precipitation_hours",
        "wind_speed_10m_max",
        "wind_gusts_10m_max"
    ]

    @staticmethod
    def fetch_weather_static(params: Dict) -> Optional[pd.DataFrame]:
        """
        Static method untuk fetch weather data - untuk multiprocessing

        Parameters:
        - params: Dict dengan keys: location_name, latitude, longitude, start_date, end_date

        Returns:
        - DataFrame dengan weather data
        """
        max_retries = 3
        retry_delay = 2  # seconds

        for attempt in range(max_retries):
            try:
                # Add small delay to avoid rate limiting
                time.sleep(0.5)

                api_params = {
                    'latitude': params['latitude'],
                    'longitude': params['longitude'],
                    'start_date': params['start_date'],
                    'end_date': params['end_date'],
                    'daily': ','.join(OpenMeteoScraper.WEATHER_VARS),
                    'timezone': 'Asia/Jakarta'
                }

                response = requests.get(
                    OpenMeteoScraper.BASE_URL,
                    params=api_params,
                    timeout=30
                )

                # Handle rate limiting
                if response.status_code == 429:
                    if attempt < max_retries - 1:
                        wait_time = retry_delay * (attempt + 1)
                        time.sleep(wait_time)
                        continue
                    else:
                        logger.warning(f"Rate limited for {params['location_name']} after {max_retries} attempts")
                        return None

                response.raise_for_status()
                data = response.json()

                if 'daily' not in data:
                    return None

                # Convert to DataFrame
                daily = data['daily']
                df = pd.DataFrame({
                    'date': pd.to_datetime(daily['time']),
                    'location_name': params['location_name'],
                    'latitude': params['latitude'],
                    'longitude': params['longitude'],
                    'temperature_max_c': daily.get('temperature_2m_max'),
                    'temperature_min_c': daily.get('temperature_2m_min'),
                    'temperature_mean_c': daily.get('temperature_2m_mean'),
                    'precipitation_mm': daily.get('precipitation_sum'),
                    'rain_mm': daily.get('rain_sum'),
                    'precipitation_hours': daily.get('precipitation_hours'),
                    'windspeed_max_kmh': daily.get('wind_speed_10m_max'),
                    'windgusts_max_kmh': daily.get('wind_gusts_10m_max'),
                    'retrieved_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                })

                return df

            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 429:
                    continue  # Will retry
                logger.error(f"HTTP Error for {params['location_name']}: {e}")
                return None
            except Exception as e:
                logger.error(f"Error fetching weather for {params['location_name']} "
                            f"({params['start_date']} to {params['end_date']}): {e}")
                return None

        # If all retries failed
        return None


def scrape_weather_for_pihps(
    pihps_file: str,
    output_file: str = 'weather_pihps_historical.csv',
    chunk_days: int = 90,
    n_workers: Optional[int] = None
):
    """
    Scrape weather data untuk semua tanggal dan lokasi di PIHPS data

    Parameters:
    - pihps_file: Path ke file PIHPS yang sudah dibersihkan
    - output_file: Path output file
    - chunk_days: Split request per berapa hari (untuk avoid timeout)
    - n_workers: Jumlah worker processes
    """

    logger.info("="*60)
    logger.info("SCRAPING WEATHER DATA FOR PIHPS")
    logger.info("="*60)

    # Load PIHPS data
    logger.info(f"Loading PIHPS data from {pihps_file}...")
    df_pihps = pd.read_csv(pihps_file)
    df_pihps['date'] = pd.to_datetime(df_pihps['date'])

    # Get unique dates
    unique_dates = sorted(df_pihps['date'].unique())
    min_date = pd.to_datetime(unique_dates[0])
    max_date = pd.to_datetime(unique_dates[-1])

    logger.info(f"Date range: {min_date.date()} to {max_date.date()}")
    logger.info(f"Total unique dates: {len(unique_dates):,}")

    # Get unique locations (exclude 'Semua Provinsi' aggregate)
    unique_locations = df_pihps[df_pihps['location_name'] != 'Semua Provinsi']['location_name'].unique()
    logger.info(f"Unique locations: {len(unique_locations)}")

    # Filter locations yang ada koordinatnya
    valid_locations = [loc for loc in unique_locations if loc in LOCATION_COORDINATES]
    logger.info(f"Locations with coordinates: {len(valid_locations)}")

    if len(valid_locations) < len(unique_locations):
        missing = set(unique_locations) - set(valid_locations)
        logger.warning(f"Missing coordinates for: {missing}")

    # Build task list - chunk date ranges
    tasks = []
    for location in valid_locations:
        lat, lon = LOCATION_COORDINATES[location]

        # Split date range into chunks
        current_start = min_date
        while current_start <= max_date:
            current_end = min(current_start + timedelta(days=chunk_days - 1), max_date)

            task = {
                'location_name': location,
                'latitude': lat,
                'longitude': lon,
                'start_date': current_start.strftime('%Y-%m-%d'),
                'end_date': current_end.strftime('%Y-%m-%d')
            }
            tasks.append(task)

            current_start = current_end + timedelta(days=1)

    total_tasks = len(tasks)
    logger.info(f"Total tasks: {total_tasks}")
    logger.info(f"({len(valid_locations)} locations × ~{(max_date - min_date).days // chunk_days + 1} chunks)")

    # Determine workers - use fewer to avoid rate limiting
    if n_workers is None:
        n_workers = min(4, cpu_count())  # Max 4 workers to avoid rate limiting

    logger.info(f"Using {n_workers} worker processes (limited to avoid rate limiting)")

    # Process tasks in parallel
    results = []
    with Pool(processes=n_workers) as pool:
        with tqdm(total=total_tasks, desc="Scraping weather data", unit="task") as pbar:
            for result in pool.imap_unordered(OpenMeteoScraper.fetch_weather_static, tasks):
                if result is not None:
                    results.append(result)
                pbar.update(1)

    logger.info(f"Successfully retrieved {len(results)} chunks of data (out of {total_tasks} tasks)")

    # Combine all results
    if results:
        logger.info("Combining all data...")
        df_weather = pd.concat(results, ignore_index=True)

        # Remove duplicates (might happen at chunk boundaries)
        df_weather = df_weather.drop_duplicates(subset=['date', 'location_name'])

        # Sort by date and location
        df_weather = df_weather.sort_values(['date', 'location_name']).reset_index(drop=True)

        # Save to CSV
        df_weather.to_csv(output_file, index=False, encoding='utf-8-sig')

        logger.info("\n" + "="*60)
        logger.info("SCRAPING COMPLETED!")
        logger.info("="*60)
        logger.info(f"Total records: {len(df_weather):,}")
        logger.info(f"Date range: {df_weather['date'].min()} to {df_weather['date'].max()}")
        logger.info(f"Locations: {df_weather['location_name'].nunique()}")
        logger.info(f"Output file: {output_file}")
        logger.info("="*60)

        # Show summary stats
        logger.info("\nWeather Summary:")
        logger.info(f"  Avg temperature: {df_weather['temperature_mean_c'].mean():.1f}°C")
        logger.info(f"  Avg precipitation: {df_weather['precipitation_mm'].mean():.1f} mm")
        logger.info(f"  Days with rain: {(df_weather['rain_mm'] > 0).sum():,} ({(df_weather['rain_mm'] > 0).sum() / len(df_weather) * 100:.1f}%)")

        return df_weather
    else:
        logger.error("No data collected")
        return None


def main():
    """Main function"""

    pihps_file = 'cleaned_pihps_data/cleaned_combined.csv'
    output_file = 'weather_pihps_historical.csv'

    if not os.path.exists(pihps_file):
        logger.error(f"PIHPS file not found: {pihps_file}")
        return 1

    # Scrape weather data
    df_weather = scrape_weather_for_pihps(
        pihps_file=pihps_file,
        output_file=output_file,
        chunk_days=90,  # 3 bulan per request
        n_workers=None  # Use all CPU cores
    )

    if df_weather is None:
        logger.error("Scraping failed")
        return 1

    # Show sample
    logger.info("\nSample data:")
    logger.info("\n" + df_weather.head(10).to_string())

    return 0


if __name__ == "__main__":
    exit(main())
