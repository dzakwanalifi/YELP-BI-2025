"""
Script untuk cleaning dan transformasi data PIHPS Bank Indonesia
- Transform dari wide format ke long format
- Bersihkan nama kolom dan nilai
- Pisahkan data per provinsi
- Perbaiki label level (Semua Provinsi, Provinsi, Kabupaten/Kota)

Author: Generated by Claude Code
Date: 2025-11-28
"""

import pandas as pd
import numpy as np
from datetime import datetime
import os
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def clean_pihps_data(input_file: str, output_dir: str = 'cleaned_pihps_data'):
    """
    Clean dan transform data PIHPS dari wide ke long format

    Parameters:
    - input_file: Path ke file CSV hasil scraping
    - output_dir: Direktori output untuk file yang sudah dibersihkan
    """

    logger.info("="*60)
    logger.info("CLEANING PIHPS DATA")
    logger.info("="*60)

    os.makedirs(output_dir, exist_ok=True)

    # Baca data
    logger.info(f"Reading data from {input_file}...")
    df = pd.read_csv(input_file, low_memory=False)
    logger.info(f"Original shape: {df.shape}")

    # Identifikasi kolom metadata dan kolom tanggal
    metadata_cols = ['no', 'name', 'level', 'commodity_id', 'commodity_name',
                     'province_id', 'province_name', 'regency_id',
                     'start_date', 'end_date', 'retrieved_at']

    # Kolom tanggal adalah semua kolom yang bukan metadata
    date_cols = [col for col in df.columns if col not in metadata_cols]
    logger.info(f"Found {len(date_cols)} date columns")

    # Melt data dari wide ke long format
    logger.info("Transforming from wide to long format...")
    df_long = pd.melt(
        df,
        id_vars=metadata_cols,
        value_vars=date_cols,
        var_name='date',
        value_name='price'
    )

    logger.info(f"After melting: {df_long.shape}")

    # Parse tanggal
    logger.info("Parsing dates...")
    df_long['date'] = pd.to_datetime(df_long['date'], format='%d/%m/%Y', errors='coerce')

    # Remove rows dengan tanggal atau harga kosong
    logger.info("Removing rows with missing dates or prices...")
    before_drop = len(df_long)
    df_long = df_long.dropna(subset=['date', 'price'])
    logger.info(f"Removed {before_drop - len(df_long):,} rows with missing data")

    # Clean price values (remove commas, convert to float)
    logger.info("Cleaning price values...")
    df_long['price'] = df_long['price'].astype(str).str.replace(',', '').str.replace('-', '')
    df_long['price'] = pd.to_numeric(df_long['price'], errors='coerce')

    # Remove rows dengan harga 0 atau negative
    df_long = df_long[df_long['price'] > 0]

    # Bersihkan level labels
    logger.info("Cleaning level labels...")

    def clean_level_label(row):
        """Bersihkan dan kategorikan level label"""
        level = row['level']
        name = row['name']

        # Level 0 = Agregat nasional/provinsi
        if level == 0:
            return 'Semua Provinsi'
        # Level 1 = Provinsi
        elif level == 1:
            return row['province_name']
        # Level 2 = Kabupaten/Kota
        elif level == 2:
            # Clean nama kota/kabupaten
            if name.startswith('Kota '):
                return name.replace('Kota ', '')
            elif name.startswith('Kabupaten '):
                return name.replace('Kabupaten ', '')
            else:
                return name
        else:
            return name

    df_long['location_name'] = df_long.apply(clean_level_label, axis=1)

    # Buat kolom location_type
    df_long['location_type'] = df_long['level'].map({
        0: 'Agregat',
        1: 'Provinsi',
        2: 'Kabupaten/Kota'
    })

    # Reorder dan rename kolom
    df_clean = df_long[[
        'date',
        'commodity_id',
        'commodity_name',
        'province_id',
        'province_name',
        'location_type',
        'location_name',
        'price',
        'retrieved_at'
    ]].copy()

    # Rename kolom ke bahasa Inggris yang lebih jelas
    df_clean.columns = [
        'date',
        'commodity_id',
        'commodity_name',
        'province_id',
        'province_name',
        'location_type',
        'location_name',
        'price',
        'retrieved_at'
    ]

    # Sort by date and location
    df_clean = df_clean.sort_values(['date', 'province_name', 'location_name', 'commodity_name'])
    df_clean = df_clean.reset_index(drop=True)

    logger.info(f"Final cleaned data shape: {df_clean.shape}")

    # Statistics
    logger.info("\n" + "="*60)
    logger.info("DATA SUMMARY")
    logger.info("="*60)
    logger.info(f"Total records: {len(df_clean):,}")
    logger.info(f"Date range: {df_clean['date'].min()} to {df_clean['date'].max()}")
    logger.info(f"Provinces: {df_clean['province_name'].unique().tolist()}")
    logger.info(f"Unique commodities: {df_clean['commodity_name'].nunique()}")
    logger.info(f"Location types: {df_clean['location_type'].value_counts().to_dict()}")

    # Save combined cleaned file
    combined_output = os.path.join(output_dir, 'cleaned_combined.csv')
    df_clean.to_csv(combined_output, index=False, encoding='utf-8-sig')
    logger.info(f"\nSaved combined cleaned file: {combined_output}")

    # Split by province dan save
    logger.info("\nSplitting by province...")
    for province_name, group in df_clean.groupby('province_name'):
        province_file = province_name.replace(' ', '_').lower()
        output_file = os.path.join(output_dir, f'{province_file}.csv')
        group.to_csv(output_file, index=False, encoding='utf-8-sig')
        logger.info(f"  - {province_name}: {len(group):,} records -> {output_file}")

    # Create summary statistics per province
    logger.info("\nCreating summary statistics...")
    summary_data = []

    for province in df_clean['province_name'].unique():
        prov_data = df_clean[df_clean['province_name'] == province]

        summary_data.append({
            'province': province,
            'total_records': len(prov_data),
            'unique_commodities': prov_data['commodity_name'].nunique(),
            'unique_locations': prov_data['location_name'].nunique(),
            'date_start': prov_data['date'].min(),
            'date_end': prov_data['date'].max(),
            'avg_price': prov_data['price'].mean(),
            'min_price': prov_data['price'].min(),
            'max_price': prov_data['price'].max()
        })

    summary_df = pd.DataFrame(summary_data)
    summary_file = os.path.join(output_dir, 'summary_statistics.csv')
    summary_df.to_csv(summary_file, index=False, encoding='utf-8-sig')
    logger.info(f"Saved summary statistics: {summary_file}")

    logger.info("\n" + "="*60)
    logger.info("CLEANING COMPLETED!")
    logger.info("="*60)

    return df_clean


def main():
    """Main function"""

    input_file = 'pihps_data/combined_jakarta_jabar_2020-11-29_2025-11-28.csv'
    output_dir = 'cleaned_pihps_data'

    if not os.path.exists(input_file):
        logger.error(f"Input file not found: {input_file}")
        return 1

    # Clean data
    df_clean = clean_pihps_data(input_file, output_dir)

    # Show sample
    logger.info("\nSample of cleaned data:")
    logger.info("\n" + df_clean.head(10).to_string())

    return 0


if __name__ == "__main__":
    exit(main())
