"""
Script untuk mengambil data PIHPS (Pusat Informasi Harga Pangan Strategis)
dari website Bank Indonesia untuk setiap komoditas di level kabupaten/kota
untuk DKI Jakarta dan Jawa Barat.

Menggunakan multiprocessing untuk mempercepat scraping.

Author: Generated by Claude Code
Date: 2025-11-28
"""

import requests
import pandas as pd
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
import time
import json
from typing import List, Dict, Optional, Tuple
import logging
from multiprocessing import Pool, cpu_count
from tqdm import tqdm
import os

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class PIHPSScraper:
    """Scraper untuk data PIHPS Bank Indonesia"""

    BASE_URL = "https://www.bi.go.id/hargapangan"

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json, text/javascript, */*; q=0.01',
            'Accept-Language': 'en-US,en;q=0.9',
            'X-Requested-With': 'XMLHttpRequest',
            'Referer': f'{self.BASE_URL}/TabelHarga/PasarTradisionalKomoditas',
        })
        self._init_session()

    def _init_session(self):
        """Inisialisasi session dengan mengakses halaman utama untuk mendapat cookies"""
        try:
            response = self.session.get(
                f'{self.BASE_URL}/TabelHarga/PasarTradisionalKomoditas',
                timeout=30
            )
            response.raise_for_status()
        except Exception as e:
            logger.error(f"Failed to initialize session: {e}")
            raise

    def get_commodities(self) -> List[Dict]:
        """Mendapatkan daftar komoditas"""
        try:
            url = f'{self.BASE_URL}/WebSite/TabelHarga/GetRefCommodityAndCategory'
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            data = response.json()
            commodities = [item for item in data['data'] if item['id'].startswith('com_')]
            return commodities
        except Exception as e:
            logger.error(f"Failed to get commodities: {e}")
            return []

    def get_provinces(self) -> List[Dict]:
        """Mendapatkan daftar provinsi"""
        try:
            url = f'{self.BASE_URL}/WebSite/TabelHarga/GetRefProvince'
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            data = response.json()
            return data['data']
        except Exception as e:
            logger.error(f"Failed to get provinces: {e}")
            return []

    def get_regencies(self, province_id: int, price_type_id: int = 1) -> List[Dict]:
        """Mendapatkan daftar kabupaten/kota untuk provinsi tertentu"""
        try:
            url = f'{self.BASE_URL}/WebSite/TabelHarga/GetRefRegency'
            params = {
                'price_type_id': price_type_id,
                'ref_prov_id': province_id
            }
            response = self.session.get(url, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
            return data['data']
        except Exception as e:
            logger.error(f"Failed to get regencies for province {province_id}: {e}")
            return []

    @staticmethod
    def get_price_data_static(params_dict: Dict) -> Optional[pd.DataFrame]:
        """
        Static method untuk get price data - digunakan untuk multiprocessing
        """
        try:
            # Create new session untuk setiap worker
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'application/json, text/javascript, */*; q=0.01',
                'Accept-Language': 'en-US,en;q=0.9',
                'X-Requested-With': 'XMLHttpRequest',
                'Referer': 'https://www.bi.go.id/hargapangan/TabelHarga/PasarTradisionalKomoditas',
            })

            # Initialize session
            session.get(
                'https://www.bi.go.id/hargapangan/TabelHarga/PasarTradisionalKomoditas',
                timeout=30
            )

            url = 'https://www.bi.go.id/hargapangan/WebSite/TabelHarga/GetGridDataKomoditas'

            api_params = {
                'price_type_id': params_dict.get('price_type_id', 1),
                'comcat_id': params_dict['commodity_id'],
                'province_id': params_dict.get('province_id', ''),
                'regency_id': params_dict.get('regency_id', ''),
                'showKota': 'true' if params_dict.get('show_kota', True) else 'false',
                'showPasar': 'true' if params_dict.get('show_pasar', False) else 'false',
                'tipe_laporan': params_dict.get('tipe_laporan', 1),
                'start_date': params_dict['start_date'],
                'end_date': params_dict['end_date']
            }

            response = session.get(url, params=api_params, timeout=60)
            response.raise_for_status()
            data = response.json()

            if not data.get('data'):
                return None

            # Convert to DataFrame
            df = pd.DataFrame(data['data'])

            # Add metadata columns
            df['commodity_id'] = params_dict['commodity_id']
            df['commodity_name'] = params_dict.get('commodity_name', '')
            df['province_id'] = params_dict.get('province_id')
            df['province_name'] = params_dict.get('province_name', '')
            df['regency_id'] = params_dict.get('regency_id')
            df['start_date'] = params_dict['start_date']
            df['end_date'] = params_dict['end_date']
            df['retrieved_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

            return df

        except Exception as e:
            logger.error(f"Error fetching {params_dict.get('commodity_name', 'unknown')} "
                        f"({params_dict['commodity_id']}) "
                        f"{params_dict['start_date']} to {params_dict['end_date']}: {e}")
            return None


def scrape_jakarta_jabar_parallel(
    start_date: str,
    end_date: str,
    output_dir: str = 'pihps_data',
    chunk_months: int = 6,
    n_workers: Optional[int] = None
):
    """
    Scrape data untuk semua komoditas di DKI Jakarta dan Jawa Barat
    menggunakan multiprocessing

    Parameters:
    - start_date: Tanggal mulai (format: YYYY-MM-DD)
    - end_date: Tanggal selesai (format: YYYY-MM-DD)
    - output_dir: Direktori untuk menyimpan hasil
    - chunk_months: Split request per berapa bulan (untuk avoid timeout)
    - n_workers: Jumlah worker processes (default: semua CPU cores)
    """
    os.makedirs(output_dir, exist_ok=True)

    # Initialize scraper untuk get metadata
    logger.info("Initializing scraper and fetching metadata...")
    scraper = PIHPSScraper()
    commodities = scraper.get_commodities()

    if not commodities:
        logger.error("Failed to get commodities list")
        return None

    logger.info(f"Found {len(commodities)} commodities")

    # Target provinces
    target_provinces = {
        'DKI Jakarta': 13,
        'Jawa Barat': 12
    }

    # Split date range into chunks
    start_dt = datetime.strptime(start_date, '%Y-%m-%d')
    end_dt = datetime.strptime(end_date, '%Y-%m-%d')

    date_ranges = []
    current_start = start_dt
    while current_start < end_dt:
        current_end = min(current_start + relativedelta(months=chunk_months), end_dt)
        date_ranges.append((
            current_start.strftime('%Y-%m-%d'),
            current_end.strftime('%Y-%m-%d')
        ))
        current_start = current_end + timedelta(days=1)

    logger.info(f"Date range split into {len(date_ranges)} chunks of ~{chunk_months} months each")

    # Build task list
    tasks = []
    for prov_name, prov_id in target_provinces.items():
        for commodity in commodities:
            for chunk_start, chunk_end in date_ranges:
                task = {
                    'commodity_id': commodity['id'],
                    'commodity_name': commodity['name'],
                    'province_id': prov_id,
                    'province_name': prov_name,
                    'start_date': chunk_start,
                    'end_date': chunk_end,
                    'show_kota': True,
                    'show_pasar': False,
                    'tipe_laporan': 1,
                    'price_type_id': 1
                }
                tasks.append(task)

    total_tasks = len(tasks)
    logger.info(f"Total tasks to process: {total_tasks}")
    logger.info(f"({len(target_provinces)} provinces × {len(commodities)} commodities × {len(date_ranges)} chunks)")

    # Determine number of workers
    if n_workers is None:
        n_workers = cpu_count()

    logger.info(f"Using {n_workers} worker processes")

    # Process tasks in parallel with progress bar
    results = []
    with Pool(processes=n_workers) as pool:
        with tqdm(total=total_tasks, desc="Scraping PIHPS data", unit="task") as pbar:
            for result in pool.imap_unordered(PIHPSScraper.get_price_data_static, tasks):
                if result is not None:
                    results.append(result)
                pbar.update(1)

    logger.info(f"Successfully retrieved {len(results)} chunks of data (out of {total_tasks} tasks)")

    # Combine all results
    if results:
        logger.info("Combining all data...")
        combined_df = pd.concat(results, ignore_index=True)

        # Group by province and commodity to save individual files
        logger.info("Saving individual files by province and commodity...")
        for (prov_name, com_id, com_name), group_df in tqdm(
            combined_df.groupby(['province_name', 'commodity_id', 'commodity_name']),
            desc="Saving files"
        ):
            filename = f"{output_dir}/{prov_name.replace(' ', '_')}_{com_id}_{start_date}_{end_date}.csv"
            group_df.to_csv(filename, index=False, encoding='utf-8-sig')

        # Save combined file
        combined_filename = f"{output_dir}/combined_jakarta_jabar_{start_date}_{end_date}.csv"
        combined_df.to_csv(combined_filename, index=False, encoding='utf-8-sig')

        logger.info("\n" + "="*60)
        logger.info("SCRAPING COMPLETED SUCCESSFULLY")
        logger.info("="*60)
        logger.info(f"Total records: {len(combined_df):,}")
        logger.info(f"Unique commodities: {combined_df['commodity_name'].nunique()}")
        logger.info(f"Unique provinces: {combined_df['province_name'].nunique()}")
        logger.info(f"Date range: {start_date} to {end_date}")
        logger.info(f"Combined file: {combined_filename}")
        logger.info(f"Output directory: {output_dir}/")
        logger.info("="*60)

        return combined_df
    else:
        logger.error("No data collected")
        return None


def main():
    """Main function untuk menjalankan scraper"""

    # Set date range (5 tahun terakhir)
    end_date = datetime.now().strftime('%Y-%m-%d')
    start_date = (datetime.now() - timedelta(days=5*365)).strftime('%Y-%m-%d')

    logger.info("="*60)
    logger.info("PIHPS SCRAPER - PARALLEL VERSION")
    logger.info("="*60)
    logger.info(f"Date range: {start_date} to {end_date}")
    logger.info(f"Target provinces: DKI Jakarta, Jawa Barat")
    logger.info(f"Output directory: pihps_data/")
    logger.info(f"CPU cores available: {cpu_count()}")
    logger.info("="*60)

    # Jalankan scraping
    df = scrape_jakarta_jabar_parallel(
        start_date=start_date,
        end_date=end_date,
        output_dir='pihps_data',
        chunk_months=6,
        n_workers=None  # Use all CPU cores
    )

    if df is None:
        logger.error("Scraping failed - no data collected")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
